{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Combined_Test.ipynb","provenance":[],"collapsed_sections":["QlHu0wxh1prP","DfQPgu1Q1prS","p_d5uuAY1prZ","fdLNA0ce1prd"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"z3p8N43E1ppt","colab":{}},"source":["import torch\n","import torch.utils.data as data\n","from torchvision import datasets, transforms\n","\n","import os\n","import os.path\n","\n","import json\n","\n","from PIL import Image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3oJqJMVXDgM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1596559514967,"user_tz":240,"elapsed":18290,"user":{"displayName":"Forrest Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLuGRQbF3xB_p_JolTbgiKEVDgxVHg4MryyevRQQ=s64","userId":"09746212145060308592"}},"outputId":"996fba3a-8d34-4bf8-f4d2-3e863f405192"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BM7oBFUlAt_t","colab_type":"code","colab":{}},"source":["!cp -r /content/drive/'My Drive'/APS360/combined_model/ /content/."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iffEzmd0XIoN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1596134402582,"user_tz":240,"elapsed":8530,"user":{"displayName":"Forrest Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLuGRQbF3xB_p_JolTbgiKEVDgxVHg4MryyevRQQ=s64","userId":"09746212145060308592"}},"outputId":"ebe42743-67ec-448a-e460-18e5fc2fe405"},"source":["!pip install gdown\n","!gdown --id 1cN4dAXKQW-cZAqcaKXXHoLqzt9YEOBXr --output combined_model_test.zip "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n","Downloading...\n","From: https://drive.google.com/uc?id=1F03OZ0UhaeMkbsEWYA1UHfeU2jRDLip7\n","To: /content/combined_model_test.zip\n","86.9MB [00:01, 52.1MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9UiVnwYpXSIR","colab_type":"code","colab":{}},"source":["!unzip -q combined_model_test.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"avkAFXRxXcO6","colab_type":"code","colab":{}},"source":["!unzip -q combined_model_select.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"354acvZHvrl3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1596134472325,"user_tz":240,"elapsed":19918,"user":{"displayName":"Forrest Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLuGRQbF3xB_p_JolTbgiKEVDgxVHg4MryyevRQQ=s64","userId":"09746212145060308592"}},"outputId":"32634322-4e0d-4169-a865-ba94d4933726"},"source":["# Note, newest version of opencv may have issues, please use\n","!pip uninstall opencv-python\n","!pip install opencv-python==4.1.2.30\n","# to get opencv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Uninstalling opencv-python-4.1.2.30:\n","  Would remove:\n","    /usr/local/lib/python3.6/dist-packages/cv2/*\n","    /usr/local/lib/python3.6/dist-packages/opencv_python-4.1.2.30.dist-info/*\n","Proceed (y/n)? y\n","  Successfully uninstalled opencv-python-4.1.2.30\n","Collecting opencv-python==4.1.2.30\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/a9/9828dfaf93f40e190ebfb292141df6b7ea1a2d57b46263e757f52be8589f/opencv_python-4.1.2.30-cp36-cp36m-manylinux1_x86_64.whl (28.3MB)\n","\u001b[K     |████████████████████████████████| 28.3MB 148kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python==4.1.2.30) (1.18.5)\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: opencv-python\n","Successfully installed opencv-python-4.1.2.30\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lHdDoHYzvgqX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596564458340,"user_tz":240,"elapsed":862,"user":{"displayName":"Forrest Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLuGRQbF3xB_p_JolTbgiKEVDgxVHg4MryyevRQQ=s64","userId":"09746212145060308592"}}},"source":["\n","import glob\n","import os\n","import random\n","import timeit\n","\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from PIL import Image, ImageDraw, ImageFont\n","\n","torch.manual_seed(1)\n","\n","# Class Wrapper for YOLO bbox generator and localization\n","class SignDetector(object):\n","    def __init__(self, cfg_path, weights_path):\n","        \"\"\"Constructor reads a YOLO cfg_path and weights_path\n","\n","        Parameters\n","        ----------\n","        cfg_path : string\n","            YOLO cfg filepath\n","        weights_path : string\n","            YOLO weights filepath\n","        \"\"\"\n","        self.cfg_path = os.path.abspath(cfg_path)\n","        self.weights_path = os.path.abspath(weights_path)\n","\n","        self.net = cv2.dnn.readNet(self.weights_path, self.cfg_path)\n","        self.layer_names = self.net.getLayerNames()\n","        self.output_layers = [self.layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n","\n","    def __call__(self, *input, **kwargs):\n","        \"\"\"Enables forward function to be called as Object(input)\n","\n","        Returns\n","        -------\n","        img : cv2 image\n","            input image to generate bbox for.\n","        \"\"\"\n","        return self.forward(*input, **kwargs)\n","\n","    def forward(self, img):\n","        \"\"\"Calculates the bbox for a cv2 image\n","\n","        Parameters\n","        ----------\n","        img : cv2 image\n","            input image\n","\n","        Returns\n","        -------\n","        list of tuples\n","            list of tuples, each tuple represents a bbox as (x_top_left, y_top_left, width, height)\n","        \"\"\"\n","        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n","        height, width, channels = img.shape\n","\n","        # Create a blob which acts as input to our model\n","        blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n","\n","        # Run the Darknet algorithm\n","        start = timeit.default_timer()\n","        self.net.setInput(blob)\n","        outs = self.net.forward(self.output_layers)\n","        stop = timeit.default_timer()\n","        #print(\"Yolo took {} to run\".format(stop-start))\n","\n","        # Process the output from darknet.\n","        confidences = []\n","        boxes = []\n","        for out in outs:\n","            for detection in out:\n","                scores = detection[5:]\n","                class_id = np.argmax(scores)\n","                confidence = scores[class_id]\n","                if confidence > 0.3:\n","                    # Object detected\n","                    center_x = int(detection[0] * width)\n","                    center_y = int(detection[1] * height)\n","                    w = int(detection[2] * width)\n","                    h = int(detection[3] * height)\n","\n","                    # Rectangle coordinates\n","                    x = int(center_x - w / 2)\n","                    y = int(center_y - h / 2)\n","\n","                    boxes.append((x, y, w, h))\n","                    confidences.append(float(confidence))\n","\n","        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","        return [boxes[x[0]] for x in indexes]\n","\n","class SignClassifier(nn.Module):\n","    def __init__(self):\n","        super(SignClassifier, self).__init__()\n","        self.name = \"signclassifierAdvancedWithDropout\"\n","        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n","        self.fc2 = nn.Linear(4096,1000)\n","        self.fc3 = nn.Linear(1000,75)\n","        self.dropout1 = nn.Dropout(0.5) # 50% dropout rate\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.dropout3 = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 256 * 6 * 6) #flatten feature data\n","        x = F.relu(self.fc1(self.dropout1(x)))\n","        x = F.relu(self.fc2(self.dropout2(x)))\n","        x = self.fc3(self.dropout3(x))\n","        return x\n","\n","class TrafficSignRecognizer(object):\n","    def __init__(self, cfg_path, weights_path, classifier_state, index_to_classes, stride=256, size=416, use_cuda=False):\n","        \"\"\"Constructor for the model which does bbox detection and classification of traffic signs\n","\n","        Parameters\n","        ----------\n","        cfg_path : string\n","            path to YOLO cfg file\n","        weights_path : string\n","            path to YOLO weights file\n","        classifier_state : string\n","            path to classifier torch model save file\n","        index_to_classes : string\n","            path to file containing mapping of classifier output index to classname\n","        use_cuda : bool, optional\n","            Turn on CUDA for model, currently not working, by default False\n","        \"\"\"\n","        self.stride = stride\n","        self.size = size\n","\n","        self.detector = SignDetector(cfg_path, weights_path)\n","        self.classifier = SignClassifier()\n","\n","        # Load the pretrained state into our classifier and set to eval mode to ignore dropout\n","        state = torch.load(classifier_state, map_location=torch.device('cpu'))\n","        self.classifier.load_state_dict(state)\n","        self.classifier.eval()\n","\n","        # alexnet pretrained model for feature extraction\n","        import torchvision.models\n","        self.alexnet = torchvision.models.alexnet(pretrained=True)\n","\n","        # map the output of classifier to a classname\n","        self.index_to_class = list()\n","        with open(index_to_classes, \"r\") as fp:\n","            for line in fp:\n","                self.index_to_class.append(line.strip())\n","\n","        # define the transformations for our classifier\n","        self.transform = transforms.Compose(\n","                [transforms.Resize((224,224)),\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","    def __get_bboxes(self, img):\n","        # img = Image.open(image_file)\n","        bboxes = self.detector(img)\n","        res = []\n","        for bbox in bboxes:\n","            x1 = bbox[0]\n","            y1 = bbox[1]\n","            x2 = x1 + bbox[2]\n","            y2 = y1 + bbox[3]\n","            res.append(torch.tensor((x1, y1, x2, y2)))\n","        return res\n","\n","    def __break_up_image(self, img):\n","        # Calc Image size\n","        imgwidth, imgheight = img.size\n","        index_to_bbox = []\n","\n","        # Crop\n","        index = 0\n","        for i in range(0, imgheight, self.stride):\n","            for j in range(0, imgwidth, self.stride):\n","                # Out of bounds!\n","                if j+self.size >= imgwidth or i+self.size >= imgheight:\n","                    continue\n","                crop_box = (j, i, j+self.size, i+self.size)\n","                cropped = img.crop(crop_box)\n","                index_to_bbox.append((torch.tensor(crop_box), cropped))\n","                index +=1\n","        return index_to_bbox\n","\n","    def __call__(self, *input, **kwargs):\n","        return self.forward(*input, **kwargs)\n","\n","    def forward(self, img, iou_threshold=0.25):\n","        img = img.convert('RGB')\n","        # Map the crop index to bbox based on original image that specifies that crop\n","        index_to_bbox = self.__break_up_image(img)\n","\n","        yolobbox_tensor = list()\n","        for index in range(len(index_to_bbox)):\n","            crop_bbox, crop_img = index_to_bbox[index]\n","            # Get the bbox from yolo\n","            yolo_bboxes = self.__get_bboxes(crop_img)\n","\n","            # Change a relative bbox to absolute bbox\n","            for yolo_bbox in yolo_bboxes:\n","                abs_bbox = [yolo_bbox[0]+crop_bbox[0],\n","                            yolo_bbox[1]+crop_bbox[1],\n","                            yolo_bbox[2]+crop_bbox[0],\n","                            yolo_bbox[3]+crop_bbox[1]]\n","\n","                #Check if box is >= 150 pixels\n","                abs_width = (abs_bbox[2] - abs_bbox[0]) * (abs_bbox[3] - abs_bbox[1])\n","                if (abs_width >= 150):\n","                  yolobbox_tensor.append(abs_bbox)\n","\n","\n","\n","        if not yolobbox_tensor:\n","            print(\"No bounding boxes\")\n","            return\n","\n","        # Convert list of bbox to torch tensor and run nms on the bboxes to merge\n","        yolobbox_tensor = torch.tensor(yolobbox_tensor, dtype=torch.float)\n","        kept_yolobbox = torchvision.ops.nms(yolobbox_tensor, torch.tensor([1.0] * yolobbox_tensor.shape[0]), iou_threshold)\n","\n","        # kept is a list of indexes that correspond to a bbox in bbox_tensor\n","        # We want to generate a return which is: [(bbox, class, probability)]\n","        res = list()\n","        for index in kept_yolobbox:\n","            bbox = [int(x) for x in yolobbox_tensor[index]]\n","            cropped = img.crop(bbox)\n","\n","            # Prepare the cropped yolo bbox for input to our classifier\n","            cropped = self.transform(cropped).float()\n","            cropped = cropped.unsqueeze(0)\n","            features = self.alexnet.features(cropped)\n","            features = torch.from_numpy(features.detach().numpy())\n","\n","            # Run the classifier on the cropped yolo bbox\n","            output = F.softmax(self.classifier(features), dim=1)\n","            prediction = torch.argmax(output)\n","            probability = torch.max(output)\n","            res.append((int(prediction), float(probability), bbox))\n","        return res\n","\n","    def get_class_name_from_index(self, index):\n","        return self.index_to_class[index]\n","\n","    def annotate_image(self, img, annotation_items):\n","        img = img.convert('RGB')\n","        drawing = ImageDraw.Draw(img)\n","\n","        for prediction, probability, bbox in annotation_items:\n","            label = self.get_class_name_from_index(prediction)\n","            drawing.rectangle(bbox, outline=(252, 3, 177), width=2)\n","            try:\n","                font = ImageFont.truetype(\"/content/combined_model/font.ttf\", 20)\n","            except:\n","                raise Exception(\"Current directory missing font.ttf for use in drawing labels on final image.\")\n","            drawing.text((bbox[0], bbox[1]-25), label, font=font,fill='white',stroke_width=1,stroke_fill='black')\n","        return img\n","\n","\n"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uX7N_WI8gwP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596150420807,"user_tz":240,"elapsed":16024,"user":{"displayName":"Forrest Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLuGRQbF3xB_p_JolTbgiKEVDgxVHg4MryyevRQQ=s64","userId":"09746212145060308592"}},"outputId":"9482535f-29ab-4754-ae8c-9fb35524a80b"},"source":["########################################\n","# Example Run + How to Use\n","\n","# YOLO config and pretrained weights\n","cfg_filename = \"/content/combined_model/yolov3_training.cfg\"\n","weights_filename = \"/content/combined_model/yolov3_training_5000.weights\"\n","state_filename = \"/content/combined_model/model_signclassifierAdvancedWithDropout_bs256_lr3e-05_wd0.0001_dropout50_epoch66\"\n","index_filename = \"/content/combined_model/index_to_class.txt\"\n","input_image = \"/content/combined_model_test/-4gtcRinmMXt0PgT-ROzMg.jpg\"\n","\n","start = timeit.default_timer()\n","\n","# Run the merged model\n","model = TrafficSignRecognizer(cfg_filename, weights_filename, state_filename, index_filename, stride=416)\n","img = Image.open(input_image)\n","output = model(img)\n","\n","# Debug\n","print(output)\n","\n","# Save the output annotated file\n","#if (len(output) > 0):\n","#  img = model.annotate_image(img, output)\n","#out_path = os.path.join(\"combined_model_output\", \"annotated_\" + os.path.split(input_image)[1])\n","#img.save(out_path)\n","\n","\n","\n","\n","stop = timeit.default_timer()\n","print(\"Took {} to run\".format(stop-start))\n","########################################"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[557, 519, 567, 535]\n","[1139, 535, 1161, 555]\n","[(22, 0.1720220297574997, [557, 519, 567, 535]), (43, 0.9911988377571106, [1139, 535, 1161, 555])]\n","Took 15.717458602997795 to run\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WAuEJUy-C25E","colab_type":"code","colab":{}},"source":["index_to_class = list()\n","with open(index_filename, \"r\") as fp:\n","  for line in fp:\n","    index_to_class.append(line.strip())\n","\n","#print (index_to_class[output[0][0]])\n","test_dict = {}\n","for i in output:\n","  if index_to_class[i[0]] in test_dict:\n","    count = test_dict[index_to_class[i[0]]]\n","    test_dict[index_to_class[i[0]]] = count + 1\n","  else:\n","    test_dict[index_to_class[i[0]]] = 1\n","\n","print (test_dict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ct0uNE5QFszP","colab_type":"text"},"source":["**Run Combined Test**"]},{"cell_type":"code","metadata":{"id":"K8TgKs-PhHu2","colab_type":"code","colab":{}},"source":["def calc_combined_acc (test_list):\n","\n","  TP = 0\n","  FP = 0\n","  FN = 0\n","\n","  class_statistic_dict = {}\n","\n","  cfg_filename = \"/content/combined_model/yolov3_training.cfg\"\n","  weights_filename = \"/content/combined_model/yolov3_training_5000.weights\"\n","  state_filename = \"/content/combined_model/model_signclassifierAdvancedWithDropout_bs256_lr3e-05_wd0.0001_dropout50_epoch66\"\n","  index_filename = \"/content/combined_model/index_to_class.txt\"\n","\n","  index_to_class = list()\n","  with open(\"/content/combined_model/index_to_class_full_name.txt\", \"r\") as fp:\n","    for line in fp:\n","      index_to_class.append(line.strip())\n","\n","  for i in index_to_class:\n","    class_statistic_dict[i + \"_TP\"] = 0\n","    class_statistic_dict[i + \"_FP\"] = 0\n","    class_statistic_dict[i + \"_FN\"] = 0\n","\n","  for image_file, txt_file in test_list:\n","    img = Image.open(image_file)\n","    label_dict = {}\n","    pred_dict = {}\n","\n","    #Get labels\n","    with open(txt_file, \"r\") as txt:\n","        for line in txt:\n","          line_split = line.split()\n","          #print (line_split)\n","          if line_split[0] in label_dict:\n","            count = label_dict[line_split[0]]\n","            label_dict[line_split[0]] = count + 1\n","          else:\n","            label_dict[line_split[0]] = 1\n","\n","  \n","   ###### Feed image into model here #########\n","    # Model outputs a dictionary - with key = sign label, and value = number occurance\n","\n","    #pred_dict = model ...\n","    #pred_dict = label_dict.copy()\n","\n","    start = timeit.default_timer()\n","\n","    # Run the merged model\n","    model = TrafficSignRecognizer(cfg_filename, weights_filename, state_filename, index_filename, stride=416)\n","    output = model(img)\n","\n","    # Debug\n","    print(output)\n","\n","    # Save the output annotated file\n","    if (output != None):\n","      img = model.annotate_image(img, output)\n","      #Get predictions\n","      for i in output:\n","        #Check if output is >= 0.5\n","        if (i[1] > 0.3):\n","          if index_to_class[i[0]] in pred_dict:\n","            count = pred_dict[index_to_class[i[0]]]\n","            pred_dict[index_to_class[i[0]]] = count + 1\n","          else:\n","            pred_dict[index_to_class[i[0]]] = 1\n","    out_path = os.path.join(\"combined_model_output\", \"annotated_\" + os.path.split(image_file)[1])\n","    img.save(out_path)\n","\n","\n","    \n","\n","    stop = timeit.default_timer()\n","    print(\"Took {} to run\".format(stop-start))\n","\n","  \n","\n","    ############################################\n","  \n","    for x in pred_dict:\n","      if (x in label_dict):\n","        if (pred_dict[x] == label_dict[x]):\n","          TP = TP + pred_dict[x]\n","          class_statistic_dict[x + \"_TP\"] = class_statistic_dict[x + \"_TP\"] + pred_dict[x]\n","\n","        elif (pred_dict[x] < label_dict[x]):\n","          #We predicted less than expected\n","          TP = TP + pred_dict[x]\n","          class_statistic_dict[x + \"_TP\"] = class_statistic_dict[x + \"_TP\"] + pred_dict[x]\n","\n","          FN = FN + label_dict[x] - pred_dict[x]\n","          class_statistic_dict[x + \"_FN\"] = class_statistic_dict[x + \"_FN\"] + label_dict[x] - pred_dict[x]\n","\n","        elif (pred_dict[x] > label_dict[x]):\n","          #We predicted more than expected\n","          TP =  TP + label_dict[x]\n","          class_statistic_dict[x + \"_TP\"] = class_statistic_dict[x + \"_TP\"] + label_dict[x]\n","\n","          FP = FP + pred_dict[x] - label_dict[x]\n","          class_statistic_dict[x + \"_FP\"] = class_statistic_dict[x + \"_FP\"] + pred_dict[x] - label_dict[x]\n","\n","        #Remove the key from the label_dict\n","        del label_dict[x]\n","      else:\n","        #We predicted something that was not there\n","        FP = FP + pred_dict[x]\n","        class_statistic_dict[x + \"_FP\"] = class_statistic_dict[x + \"_FP\"] + pred_dict[x]\n","\n","    #Count remaining signs that we missed\n","    for x in label_dict:\n","      FN = FN + label_dict[x]\n","      class_statistic_dict[x + \"_FN\"] = class_statistic_dict[x + \"_FN\"] + label_dict[x]\n","\n","    print (\"TP, FN, FP: \", TP, \" \", FN, \" \", FP)\n","\n","  #Avoid dividing by 0 - should be impossible\n","  precision = 0\n","  if ((TP + FP ) > 0):\n","    precision = TP / (TP + FP)\n","\n","  recall = 0\n","  if ((TP + FN) > 0):\n","    recall = TP / (TP + FN)\n","  \n","  acc = 0\n","  if ((TP + FN + FP) > 0):\n","    acc = TP / (TP + FN + FP)\n","\n","  \n","\n","  print (\"Precision: \", precision)\n","  print (\"Recall: \", recall)\n","  print (\"Total Accuracy: \", acc)\n","\n","  return class_statistic_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0j5tZd17S8Ti","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":292},"executionInfo":{"status":"ok","timestamp":1596564537945,"user_tz":240,"elapsed":64318,"user":{"displayName":"Forrest Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLuGRQbF3xB_p_JolTbgiKEVDgxVHg4MryyevRQQ=s64","userId":"09746212145060308592"}},"outputId":"113db22a-5d02-43da-87ae-4a83c655c930"},"source":["import glob\n","\n","!rm -rf combined_model_output/\n","!mkdir combined_model_output/\n","\n","label_list = sorted(glob.glob(\"/content/combined_model_test/*.txt\"))\n","images_list = sorted(glob.glob(\"/content/combined_model_test/*.jpg\"))\n","result_dict = calc_combined_acc (zip(images_list, label_list))\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["[(22, 0.17202192544937134, [557, 519, 567, 535]), (43, 0.9911988377571106, [1139, 535, 1161, 555])]\n","Took 16.51930267300031 to run\n","TP, FN, FP:  1   1   0\n","[(2, 0.8632381558418274, [403, 640, 414, 666]), (4, 0.5482599139213562, [416, 639, 424, 666]), (34, 0.9997616410255432, [980, 608, 1010, 637])]\n","Took 15.35829008500059 to run\n","TP, FN, FP:  2   1   2\n","[(58, 0.9997319579124451, [558, 635, 581, 656]), (58, 0.9992503523826599, [770, 614, 789, 636]), (1, 0.9999995231628418, [959, 589, 997, 636]), (1, 0.9999994039535522, [1024, 577, 1073, 641]), (1, 0.9999960660934448, [1142, 558, 1208, 633])]\n","Took 14.435903274999873 to run\n","TP, FN, FP:  4   1   5\n","[(48, 1.0, [82, 293, 157, 393]), (43, 0.9938702583312988, [164, 279, 271, 342]), (48, 0.5298095941543579, [694, 553, 705, 567])]\n","Took 14.55797336699925 to run\n","TP, FN, FP:  5   1   7\n","Precision:  0.4166666666666667\n","Recall:  0.8333333333333334\n","Total Accuracy:  0.38461538461538464\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s8HaUnruSlRr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1596155648456,"user_tz":240,"elapsed":339,"user":{"displayName":"Forrest Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLuGRQbF3xB_p_JolTbgiKEVDgxVHg4MryyevRQQ=s64","userId":"09746212145060308592"}},"outputId":"93d817ed-4d9d-48c4-cdbb-6fc2361f9963"},"source":["print (result_dict)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'complementary--both-directions--g1_TP': 0, 'complementary--both-directions--g1_FP': 1, 'complementary--both-directions--g1_FN': 0, 'complementary--chevron-left--g1_TP': 5, 'complementary--chevron-left--g1_FP': 7, 'complementary--chevron-left--g1_FN': 0, 'complementary--chevron-right--g1_TP': 6, 'complementary--chevron-right--g1_FP': 9, 'complementary--chevron-right--g1_FN': 0, 'complementary--obstacle-delineator--g1_TP': 1, 'complementary--obstacle-delineator--g1_FP': 0, 'complementary--obstacle-delineator--g1_FN': 1, 'complementary--obstacle-delineator--g2_TP': 1, 'complementary--obstacle-delineator--g2_FP': 4, 'complementary--obstacle-delineator--g2_FN': 1, 'complementary--tow-away-zone--g1_TP': 0, 'complementary--tow-away-zone--g1_FP': 0, 'complementary--tow-away-zone--g1_FN': 0, 'information--airport--g2_TP': 0, 'information--airport--g2_FP': 0, 'information--airport--g2_FN': 0, 'information--bike-route--g1_TP': 1, 'information--bike-route--g1_FP': 0, 'information--bike-route--g1_FN': 0, 'information--disabled-persons--g1_TP': 1, 'information--disabled-persons--g1_FP': 0, 'information--disabled-persons--g1_FN': 0, 'information--gas-station--g1_TP': 0, 'information--gas-station--g1_FP': 0, 'information--gas-station--g1_FN': 0, 'information--highway-exit--g1_TP': 0, 'information--highway-exit--g1_FP': 0, 'information--highway-exit--g1_FN': 1, 'information--hospital--g1_TP': 1, 'information--hospital--g1_FP': 0, 'information--hospital--g1_FN': 0, 'information--telephone--g1_TP': 0, 'information--telephone--g1_FP': 1, 'information--telephone--g1_FN': 1, 'regulatory--do-not-block-intersection--g1_TP': 0, 'regulatory--do-not-block-intersection--g1_FP': 0, 'regulatory--do-not-block-intersection--g1_FN': 0, 'regulatory--dual-lanes-go-straight-on-left--g1_TP': 0, 'regulatory--dual-lanes-go-straight-on-left--g1_FP': 0, 'regulatory--dual-lanes-go-straight-on-left--g1_FN': 1, 'regulatory--dual-lanes-go-straight-on-right--g1_TP': 0, 'regulatory--dual-lanes-go-straight-on-right--g1_FP': 0, 'regulatory--dual-lanes-go-straight-on-right--g1_FN': 0, 'regulatory--go-straight--g3_TP': 1, 'regulatory--go-straight--g3_FP': 0, 'regulatory--go-straight--g3_FN': 0, 'regulatory--go-straight-or-turn-left--g2_TP': 1, 'regulatory--go-straight-or-turn-left--g2_FP': 1, 'regulatory--go-straight-or-turn-left--g2_FN': 0, 'regulatory--keep-left--g2_TP': 0, 'regulatory--keep-left--g2_FP': 0, 'regulatory--keep-left--g2_FN': 0, 'regulatory--keep-right--g4_TP': 2, 'regulatory--keep-right--g4_FP': 0, 'regulatory--keep-right--g4_FN': 2, 'regulatory--maximum-speed-limit-100--g3_TP': 1, 'regulatory--maximum-speed-limit-100--g3_FP': 0, 'regulatory--maximum-speed-limit-100--g3_FN': 0, 'regulatory--maximum-speed-limit-25--g2_TP': 0, 'regulatory--maximum-speed-limit-25--g2_FP': 1, 'regulatory--maximum-speed-limit-25--g2_FN': 1, 'regulatory--maximum-speed-limit-30--g3_TP': 0, 'regulatory--maximum-speed-limit-30--g3_FP': 0, 'regulatory--maximum-speed-limit-30--g3_FN': 0, 'regulatory--maximum-speed-limit-35--g2_TP': 0, 'regulatory--maximum-speed-limit-35--g2_FP': 0, 'regulatory--maximum-speed-limit-35--g2_FN': 0, 'regulatory--maximum-speed-limit-40--g3_TP': 1, 'regulatory--maximum-speed-limit-40--g3_FP': 1, 'regulatory--maximum-speed-limit-40--g3_FN': 0, 'regulatory--maximum-speed-limit-45--g3_TP': 0, 'regulatory--maximum-speed-limit-45--g3_FP': 0, 'regulatory--maximum-speed-limit-45--g3_FN': 1, 'regulatory--maximum-speed-limit-55--g2_TP': 1, 'regulatory--maximum-speed-limit-55--g2_FP': 1, 'regulatory--maximum-speed-limit-55--g2_FN': 0, 'regulatory--no-bicycles--g2_TP': 0, 'regulatory--no-bicycles--g2_FP': 0, 'regulatory--no-bicycles--g2_FN': 0, 'regulatory--no-buses--g3_TP': 0, 'regulatory--no-buses--g3_FP': 0, 'regulatory--no-buses--g3_FN': 0, 'regulatory--no-entry--g1_TP': 9, 'regulatory--no-entry--g1_FP': 5, 'regulatory--no-entry--g1_FN': 5, 'regulatory--no-heavy-goods-vehicles--g2_TP': 0, 'regulatory--no-heavy-goods-vehicles--g2_FP': 3, 'regulatory--no-heavy-goods-vehicles--g2_FN': 3, 'regulatory--no-left-turn--g2_TP': 0, 'regulatory--no-left-turn--g2_FP': 0, 'regulatory--no-left-turn--g2_FN': 0, 'regulatory--no-overtaking--g5_TP': 1, 'regulatory--no-overtaking--g5_FP': 0, 'regulatory--no-overtaking--g5_FN': 1, 'regulatory--no-parking--g2_TP': 1, 'regulatory--no-parking--g2_FP': 1, 'regulatory--no-parking--g2_FN': 1, 'regulatory--no-pedestrians--g2_TP': 1, 'regulatory--no-pedestrians--g2_FP': 1, 'regulatory--no-pedestrians--g2_FN': 0, 'regulatory--no-right-turn--g1_TP': 0, 'regulatory--no-right-turn--g1_FP': 1, 'regulatory--no-right-turn--g1_FN': 1, 'regulatory--no-straight-through--g1_TP': 0, 'regulatory--no-straight-through--g1_FP': 0, 'regulatory--no-straight-through--g1_FN': 1, 'regulatory--no-turn-on-red--g2_TP': 1, 'regulatory--no-turn-on-red--g2_FP': 1, 'regulatory--no-turn-on-red--g2_FN': 1, 'regulatory--no-u-turn--g1_TP': 2, 'regulatory--no-u-turn--g1_FP': 0, 'regulatory--no-u-turn--g1_FN': 2, 'regulatory--one-way-left--g3_TP': 2, 'regulatory--one-way-left--g3_FP': 0, 'regulatory--one-way-left--g3_FN': 0, 'regulatory--one-way-right--g3_TP': 2, 'regulatory--one-way-right--g3_FP': 1, 'regulatory--one-way-right--g3_FN': 2, 'regulatory--reversible-lanes--g2_TP': 0, 'regulatory--reversible-lanes--g2_FP': 0, 'regulatory--reversible-lanes--g2_FN': 0, 'regulatory--road-closed--g2_TP': 1, 'regulatory--road-closed--g2_FP': 1, 'regulatory--road-closed--g2_FN': 0, 'regulatory--stop--g1_TP': 5, 'regulatory--stop--g1_FP': 2, 'regulatory--stop--g1_FN': 4, 'regulatory--triple-lanes-turn-left-center-lane--g1_TP': 0, 'regulatory--triple-lanes-turn-left-center-lane--g1_FP': 1, 'regulatory--triple-lanes-turn-left-center-lane--g1_FN': 0, 'regulatory--turn-left--g2_TP': 3, 'regulatory--turn-left--g2_FP': 1, 'regulatory--turn-left--g2_FN': 2, 'regulatory--turn-right--g3_TP': 0, 'regulatory--turn-right--g3_FP': 0, 'regulatory--turn-right--g3_FN': 0, 'regulatory--wrong-way--g1_TP': 0, 'regulatory--wrong-way--g1_FP': 0, 'regulatory--wrong-way--g1_FN': 0, 'regulatory--yield--g1_TP': 11, 'regulatory--yield--g1_FP': 6, 'regulatory--yield--g1_FN': 3, 'warning--crossroads--g3_TP': 1, 'warning--crossroads--g3_FP': 0, 'warning--crossroads--g3_FN': 1, 'warning--curve-left--g2_TP': 2, 'warning--curve-left--g2_FP': 0, 'warning--curve-left--g2_FN': 2, 'warning--curve-right--g2_TP': 1, 'warning--curve-right--g2_FP': 0, 'warning--curve-right--g2_FN': 1, 'warning--divided-highway-ends--g2_TP': 2, 'warning--divided-highway-ends--g2_FP': 1, 'warning--divided-highway-ends--g2_FN': 1, 'warning--double-curve-first-left--g2_TP': 1, 'warning--double-curve-first-left--g2_FP': 0, 'warning--double-curve-first-left--g2_FN': 0, 'warning--double-curve-first-right--g2_TP': 1, 'warning--double-curve-first-right--g2_FP': 0, 'warning--double-curve-first-right--g2_FN': 1, 'warning--height-restriction--g2_TP': 1, 'warning--height-restriction--g2_FP': 1, 'warning--height-restriction--g2_FN': 0, 'warning--junction-with-a-side-road-perpendicular-left--g3_TP': 0, 'warning--junction-with-a-side-road-perpendicular-left--g3_FP': 0, 'warning--junction-with-a-side-road-perpendicular-left--g3_FN': 1, 'warning--junction-with-a-side-road-perpendicular-right--g3_TP': 1, 'warning--junction-with-a-side-road-perpendicular-right--g3_FP': 0, 'warning--junction-with-a-side-road-perpendicular-right--g3_FN': 2, 'warning--narrow-bridge--g1_TP': 3, 'warning--narrow-bridge--g1_FP': 1, 'warning--narrow-bridge--g1_FN': 1, 'warning--pedestrians-crossing--g4_TP': 2, 'warning--pedestrians-crossing--g4_FP': 1, 'warning--pedestrians-crossing--g4_FN': 2, 'warning--road-narrows--g2_TP': 0, 'warning--road-narrows--g2_FP': 0, 'warning--road-narrows--g2_FN': 0, 'warning--road-narrows-left--g2_TP': 0, 'warning--road-narrows-left--g2_FP': 4, 'warning--road-narrows-left--g2_FN': 0, 'warning--road-narrows-right--g2_TP': 0, 'warning--road-narrows-right--g2_FP': 0, 'warning--road-narrows-right--g2_FN': 1, 'warning--roundabout--g2_TP': 1, 'warning--roundabout--g2_FP': 0, 'warning--roundabout--g2_FN': 0, 'warning--school-zone--g2_TP': 0, 'warning--school-zone--g2_FP': 0, 'warning--school-zone--g2_FN': 0, 'warning--steep-descent--g2_TP': 0, 'warning--steep-descent--g2_FP': 0, 'warning--steep-descent--g2_FN': 1, 'warning--traffic-merges-left--g1_TP': 0, 'warning--traffic-merges-left--g1_FP': 1, 'warning--traffic-merges-left--g1_FN': 0, 'warning--traffic-merges-right--g1_TP': 0, 'warning--traffic-merges-right--g1_FP': 1, 'warning--traffic-merges-right--g1_FN': 0, 'warning--traffic-signals--g3_TP': 0, 'warning--traffic-signals--g3_FP': 0, 'warning--traffic-signals--g3_FN': 3, 'warning--t-roads--g2_TP': 0, 'warning--t-roads--g2_FP': 1, 'warning--t-roads--g2_FN': 1, 'warning--turn-left--g1_TP': 0, 'warning--turn-left--g1_FP': 1, 'warning--turn-left--g1_FN': 0, 'warning--turn-right--g1_TP': 2, 'warning--turn-right--g1_FP': 0, 'warning--turn-right--g1_FN': 0, 'warning--winding-road-first-left--g1_TP': 1, 'warning--winding-road-first-left--g1_FP': 0, 'warning--winding-road-first-left--g1_FN': 0, 'warning--winding-road-first-right--g1_TP': 0, 'warning--winding-road-first-right--g1_FP': 0, 'warning--winding-road-first-right--g1_FN': 1, 'warning--y-roads--g1_TP': 0, 'warning--y-roads--g1_FP': 7, 'warning--y-roads--g1_FN': 0}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qO7K0A3kJiUr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596156159770,"user_tz":240,"elapsed":331,"user":{"displayName":"Forrest Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLuGRQbF3xB_p_JolTbgiKEVDgxVHg4MryyevRQQ=s64","userId":"09746212145060308592"}},"outputId":"cabcbee1-9cc5-45ba-ef01-15c375901659"},"source":["def get_sensitivity (result_dict, sign_name):\n","\n","  nominator = result_dict[sign_name + \"_TP\"]\n","  denominator = (result_dict[sign_name + \"_TP\"] + result_dict[sign_name + \"_FN\"])\n","\n","  result = 0\n","  if (denominator > 0):\n","    result = nominator / denominator\n","\n","  return result\n","\n","  return result_dict[sign_name + \"_TP\"] / (result_dict[sign_name + \"_TP\"] + result_dict[sign_name + \"_FN\"])\n","\n","def get_precision (result_dict, sign_name):\n","\n","  nominator = result_dict[sign_name + \"_TP\"]\n","  denominator = (result_dict[sign_name + \"_TP\"] + result_dict[sign_name + \"_FP\"])\n","\n","  result = 0\n","  if (denominator > 0):\n","    result = nominator / denominator\n","    \n","  return result\n","\n","index_to_class = list()\n","with open(\"/content/combined_model/index_to_class_full_name.txt\", \"r\") as fp:\n","  for line in fp:\n","    index_to_class.append(line.strip())\n","\n","\n","most_common = \"\"\n","most_common_count = 0\n","least_common = \"\"\n","least_common_count = 1000\n","\n","most_correct = \"\"\n","most_correct_num = 0\n","least_correct = \"\"\n","least_correct_num = 1000\n","\n","for i in index_to_class:\n","  TP = result_dict[i + \"_TP\"]\n","  FP = result_dict[i + \"_FP\"]\n","  FN = result_dict[i + \"_FN\"]\n","\n","  real_count = TP + FN\n","\n","  if (real_count > most_common_count):\n","    most_common = i\n","    most_common_count = real_count\n","  \n","  if (real_count < least_common_count):\n","    least_common = i\n","    least_common_count = real_count\n","  \n","  correct = TP - FP - FN\n","\n","  if (correct > most_correct_num):\n","    most_correct = i\n","    most_correct_num = correct\n","  if (correct < least_correct_num):\n","    least_correct = i\n","    least_correct_num = correct\n","\n","print (\"most_common: \", most_common, \"count: \", most_common_count,\n","       \"sensitivity: \", get_sensitivity(result_dict, most_common),\n","       \"precision: \", get_precision(result_dict, most_common))\n","\n","\n","print (\"least common: \", least_common, \"count: \", least_correct_count,\n","       \"sensitivity: \", get_sensitivity(result_dict, least_common),\n","       \"precision: \", get_precision(result_dict, least_common))\n","\n","print (\"most_correct: \", most_correct, \"Times Correct: \", most_correct_num,\n","       \"sensitivity: \", get_sensitivity(result_dict, most_correct),\n","       \"precision: \", get_precision(result_dict, most_correct))\n","print (\"least_correct: \", least_correct, \"Times Correct: \", least_correct_num,\n","       \"sensitivity: \", get_sensitivity(result_dict, least_correct),\n","       \"precision: \", get_precision(result_dict, least_correct))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["most_common:  regulatory--no-entry--g1 count:  14 sensitivity:  0.6428571428571429 precision:  0.6428571428571429\n","least common:  complementary--both-directions--g1 count:  0 sensitivity:  0 precision:  0.0\n","most_correct:  regulatory--one-way-left--g3 Times Correct:  2 sensitivity:  1.0 precision:  1.0\n","least_correct:  warning--y-roads--g1 Times Correct:  -7 sensitivity:  0 precision:  0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V-vqv-yV0b3s","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596564563650,"user_tz":240,"elapsed":2062,"user":{"displayName":"Forrest Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLuGRQbF3xB_p_JolTbgiKEVDgxVHg4MryyevRQQ=s64","userId":"09746212145060308592"}}},"source":["!zip -q -r /content/drive/'My Drive'/APS360/combined_model/combined_model_output_selected.zip combined_model_output"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OhtOdxzd1ppr"},"source":["**Process Test Set**\n","\n","Use the code below to convert Mapillary images + annotations.\n","Resizes test set to specified dimensions and removes small signs.\n","Counts number of occurance for valid signs and saves them in a text file."]},{"cell_type":"code","metadata":{"id":"PPA_FQiHpmiR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1596139332286,"user_tz":240,"elapsed":6493,"user":{"displayName":"Forrest Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLuGRQbF3xB_p_JolTbgiKEVDgxVHg4MryyevRQQ=s64","userId":"09746212145060308592"}},"outputId":"db65d5b4-1330-489d-c584-f2c163f06234"},"source":["#Get Mapillary Dataset\n","!pip install gdown\n","!gdown --id 1J_I1NVNdUGwQs38RfbpL1ih_iyvrLt21 --output test_set.zip \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n","Downloading...\n","From: https://drive.google.com/uc?id=1J_I1NVNdUGwQs38RfbpL1ih_iyvrLt21\n","To: /content/test_set.zip\n","91.1MB [00:00, 137MB/s] \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HixWdxLRsx0e","colab_type":"code","colab":{}},"source":["!unzip -q test_set.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RU12Fnr9s2Ub","colab_type":"code","colab":{}},"source":["#Create a dataset for images/annotations\n","#Makes it easier to iterate through them\n","class AnnotationDataSet(data.Dataset):\n","    def __init__(self, path_to_imgs, path_to_json, transform = None):\n","        self.path_to_imgs = path_to_imgs\n","        self.path_to_json = path_to_json\n","        self.image_ids = os.listdir(path_to_imgs)\n","        self.transform = transform\n","\n","    def __getitem__(self, idx):\n","        img_id = self.image_ids[idx]\n","        img_id = os.path.splitext(img_id)[0]\n","        img = Image.open(os.path.join(self.path_to_imgs, img_id + \".jpg\"))\n","        json_file = json.load(open(os.path.join(self.path_to_json, img_id + \".json\")))\n","\n","\n","        return img, json_file, img_id\n","\n","    def __len__(self):\n","        return len(self.image_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qYdw10NAe-X-","colab_type":"code","colab":{}},"source":["#Resizes the image to ~1920x1080 for consistency\n","#Counts the number of signs per image.\n","#Only counts sign types that are in valid signs\n","\n","def count_signs (image_file, json_file, name, valid_signs, max_width, max_height, output_dir, discard_dir, min_sign_pixel = 150):\n","\n","      obj_dict = {}\n","      imgwidth, imgheight = image_file.size\n","\n","      #Ratios - Limit the ratios to 1 so we do not enlarge image\n","      width_ratio = max(1, imgwidth / max_width)\n","      height_ratio = max(1, imgheight / max_height)\n","\n","      #Resize by the larger ratio\n","      resize_ratio = max(width_ratio, height_ratio)\n","\n","      new_size = (int(imgwidth/resize_ratio), int(imgheight/resize_ratio))\n","      new_im = image_file.resize(new_size)\n","\n","      #Bounding box would decrease by ratio^2\n","      resize_ratio_squared = resize_ratio * resize_ratio\n","\n","       # Check which signs are large enough to detect\n","      for i in json_file[\"objects\"]:\n","\n","          width = i[\"bbox\"]['xmax'] - i[\"bbox\"]['xmin']\n","          height = i[\"bbox\"]['ymax'] - i[\"bbox\"]['ymin']\n","\n","          area_resized = width * height / resize_ratio_squared\n","\n","          if (area_resized < min_sign_pixel):\n","              continue\n","\n","          if (i[\"label\"] in valid_signs):\n","            if (i[\"label\"] in obj_dict):\n","                count = obj_dict[i[\"label\"]]\n","                obj_dict[i[\"label\"]] = count + 1\n","\n","            else:\n","                obj_dict[i[\"label\"]] = 1\n","\n","\n","\n","        \n","      if (len(obj_dict) > 0):\n","\n","        new_im.save(os.path.join(output_dir, \"%s.jpg\" % (name)))\n","        newline = 0  \n","        with open(os.path.join(output_dir, \"%s.txt\" % (name)), \"w\") as fp:\n","            for x in obj_dict:\n","              if (newline):\n","                fp.write(\"\\n\")\n","              str_content = \"{0} {1:1d}\".format(x, int(obj_dict[x]))\n","              fp.write(str_content)\n","              newline = 1\n","            fp.close()\n","      else:\n","        #print (\"Image: \", name, \" no longer has any detectable signs!\")\n","        new_im.save(os.path.join(discard_dir, \"%s.jpg\" % (name)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SwqJEvZtgP7b","colab_type":"code","colab":{}},"source":["import numpy as np\n","valid_signs = ['regulatory--no-parking--g2', 'regulatory--maximum-speed-limit-55--g2', 'warning--road-narrows-left--g2', 'regulatory--maximum-speed-limit-25--g2', 'warning--pedestrians-crossing--g4', 'warning--roundabout--g2', 'regulatory--keep-left--g2', 'regulatory--do-not-block-intersection--g1', 'regulatory--turn-left--g2', 'regulatory--stop--g1', 'warning--steep-descent--g2', 'complementary--obstacle-delineator--g1', 'regulatory--one-way-right--g3', 'warning--narrow-bridge--g1', 'warning--turn-right--g1', 'warning--t-roads--g2', 'regulatory--wrong-way--g1', 'warning--crossroads--g3', 'regulatory--maximum-speed-limit-30--g3', 'information--disabled-persons--g1', 'regulatory--go-straight--g3', 'regulatory--yield--g1', 'information--gas-station--g1', 'regulatory--no-heavy-goods-vehicles--g2', 'complementary--chevron-right--g1', 'regulatory--no-entry--g1', 'information--highway-exit--g1', 'complementary--tow-away-zone--g1', 'warning--traffic-merges-right--g1', 'regulatory--no-overtaking--g5', 'warning--junction-with-a-side-road-perpendicular-right--g3', 'warning--road-narrows--g2', 'regulatory--no-right-turn--g1', 'warning--turn-left--g1', 'warning--school-zone--g2', 'regulatory--no-straight-through--g1', 'information--hospital--g1', 'regulatory--no-u-turn--g1', 'regulatory--one-way-left--g3', 'warning--road-narrows-right--g2', 'regulatory--maximum-speed-limit-45--g3', 'complementary--both-directions--g1', 'regulatory--no-bicycles--g2', 'warning--winding-road-first-left--g1', 'regulatory--dual-lanes-go-straight-on-right--g1', 'regulatory--triple-lanes-turn-left-center-lane--g1', 'warning--traffic-merges-left--g1', 'warning--traffic-signals--g3', 'warning--y-roads--g1', 'information--telephone--g1', 'complementary--chevron-left--g1', 'regulatory--reversible-lanes--g2', 'warning--winding-road-first-right--g1', 'warning--double-curve-first-right--g2', 'regulatory--maximum-speed-limit-100--g3', 'regulatory--maximum-speed-limit-35--g2', 'regulatory--no-buses--g3', 'information--airport--g2', 'warning--curve-left--g2', 'warning--curve-right--g2', 'regulatory--turn-right--g3', 'warning--junction-with-a-side-road-perpendicular-left--g3', 'warning--double-curve-first-left--g2', 'regulatory--no-turn-on-red--g2', 'regulatory--dual-lanes-go-straight-on-left--g1', 'regulatory--no-left-turn--g2', 'regulatory--road-closed--g2', 'warning--divided-highway-ends--g2', 'warning--height-restriction--g2', 'complementary--obstacle-delineator--g2', 'regulatory--maximum-speed-limit-40--g3', 'information--bike-route--g1', 'regulatory--no-pedestrians--g2', 'regulatory--keep-right--g4', 'regulatory--go-straight-or-turn-left--g2']\n","test_set = AnnotationDataSet(\"/content/combined_model_select/images\", \"/content/combined_model_select/annotations\")\n","\n","for img, json_file, img_id in iter(test_set):\n","  count_signs (img, json_file, img_id, valid_signs, 1920, 1080, \"/content/combined_model_test\", \"/content/discard\", min_sign_pixel = 150)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_uM2wLAQ9l_","colab_type":"code","colab":{}},"source":["!rm -rf /content/combined_model_test/\n","!mkdir /content/combined_model_test/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ci142wJWhHjH","colab_type":"code","colab":{}},"source":["!zip -q -r /content/drive/'My Drive'/APS360/combined_model/combined_model_test.zip combined_model_test   "],"execution_count":null,"outputs":[]}]}